<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="2021/06/29/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/"/>
      <url>2021/06/29/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<p>为了确定我们的多元回归模型是否显著，需要用到假设检验来对模型进行推断。常用的假设检验有t检验、F检验和LM检验等。</p><span id="more"></span><h2 id="多元回归模型：推断"><a href="#多元回归模型：推断" class="headerlink" title="多元回归模型：推断"></a>多元回归模型：推断</h2><h3 id="正态性假定"><a href="#正态性假定" class="headerlink" title="正态性假定"></a>正态性假定</h3><ul><li>MLR6（正态性假定）：假设总体的误差是呈正态分布的；</li></ul><script type="math/tex; mode=display">E(u|x_1,\cdots,x_k)=E(u)=0\textbf{且}Var(u)=\sigma^2</script><p>MLR1-MLR6被称为经典线性模型假定（CLM），有了正态性假定，就可以构建t统计量，从而检验系数的显著水平。</p><p>定理一：在MLR1-MLR6下，有$\hat\beta_j\sim Normal(\beta_j,Var(\hat \beta_j))$，对系数标准化，就有$(\hat \beta_j-\beta_j)/se(\hat\beta_j)\sim N(0,1)$，小样本时，$(\hat \beta_j-\beta_j)/se(\hat\beta_j)\sim t_{df}=t_{n-k-1}$.</p><p>这样，我们就能构建判断系数是否显著的t统计量，若t值是统计显著的，那么我们就可以说$x_j$是一个还不错的自变量。具体的推断需要用到假设检验。</p><h3 id="t检验"><a href="#t检验" class="headerlink" title="t检验"></a>t检验</h3><p>原假设$H_0:\beta_j=0$;   备择假设$H_1:\beta_j\neq0$</p><p>t统计量：$t_{\hat\beta_j}=\hat\beta_j/se(\hat\beta_j)$,它度量了被估计的标准差于零相差多大，t越远离零，越能够拒绝原假设，t越接近零，越不能拒绝原假设。</p><p>若$|t|&gt;c$则可以在$\alpha/2$下的显著水平下拒绝原假设（c为在含n-k-1个自由度的t分布的第$1-\alpha$分位数）</p><p>同样的，我们也能够求出p值——$P(|T|&gt;|t|)$,也就是拒绝域的面积之和。t值越大越容易拒绝原假设，p值越小越容易拒绝原假设。</p><h3 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h3><p>一个95%的置信区间为：[$\hat\beta_j-c\cdot se(\hat\beta_j),\hat\beta_j+c\cdot se(\hat\beta_j)$]其中c为$t_{n-k-1}$分布的第97.5个百分数。</p><h3 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a>F检验</h3><p>t检验是对回归系数的一个线性假设的检验，有时候我们需要对多个回归系数的线性假设进行检验，这就需要用到F检验。</p><p>$H_0:\beta_3=\beta_4=\beta_5=0$，$H_1:\textbf{至少有一个为零}$</p><p>构建F统计量：$F=\frac{SSR_r-SSR_{ur}/q}{SSR_{ur}/(n-k-1)}=\frac{(R_{ur}^2-R_r^2)/q}{(1-R_{ur}^2)/df_{ur}}\sim F_{q,n-k-1}$</p><p>q:分子自由度；n-k-1：分母自由度，F值越大越能够拒绝$H_0$而接受$H_1$。当$df\rightarrow \infty$时，5%的临界值$c=2.76$.</p>]]></content>
      
      
      <categories>
          
          <category> 计量经济学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯——马尔可夫假定</title>
      <link href="2021/05/17/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%AB%98%E6%96%AF-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%81%87%E5%AE%9A/"/>
      <url>2021/05/17/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%AB%98%E6%96%AF-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%81%87%E5%AE%9A/</url>
      
        <content type="html"><![CDATA[<p>简单回归分析中有一个很大的缺陷就是很难得到在其他条件不变时x对y的影响，因为此时零条件均值假定往往并不成立。这时我们需要用到更加准确的回归方法——多元线性回归<br><!---more---></p>]]></content>
      
      
      <categories>
          
          <category> 计量经济学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多元回归模型（一）</title>
      <link href="2021/03/19/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>2021/03/19/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<p>简单回归模型可以用来研究两个变量之间的关系。出于我们就将会看到的原因，简单回归模型要作为经验分析的一般工具，还存在着局限性。不过有时把它作为一个经验工具还是很合适的。</p><span id="more"></span><p>为什么要做多元回归分析？</p><ul><li><p>简单回归分析难以做到保持其它相关条件不变；</p></li><li><p>存在多个自变量时，估计是有偏的。</p></li></ul><h2 id="多元回归模型"><a href="#多元回归模型" class="headerlink" title="多元回归模型"></a>多元回归模型</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>多元线性回归模型：</p><script type="math/tex; mode=display">Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_kX_k+u</script><p>简单回归模型难以做到保持其他相关条件不变，如果影响因变量的因素不止一个，就需要用到多元回归模型。</p><p><strong>零条件均值假定</strong>：$E(u|X_1,X_2,\cdots,X_k)=E(u)=0$</p><p>总体回归函数为：$E(Y|X_1,\cdots,X_k)=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_kX_k$</p><p>其中$X_1$增加$\triangle X_1$的影响为：$\triangle E(Y|X_2,\cdots,X_k)=\beta_1\triangle X_1$</p><p>$X_1$增加一个单位的影响为：$\triangle E(Y|X_2,\cdots,X_k)=\beta_1$</p><p>多元回归分析在非实验环境中模拟了受控试验，即回归系数反映了保持其他解释变量不变的情况下，某个解释变量对被解释变量的影响</p><h3 id="OLS估计"><a href="#OLS估计" class="headerlink" title="OLS估计"></a>OLS估计</h3><h4 id="OLS估计方法"><a href="#OLS估计方法" class="headerlink" title="OLS估计方法"></a>OLS估计方法</h4><p><strong>样本回归函数</strong>：$\hat Y=\hat \beta_0+\hat \beta_1X_1+\hat \beta_2X_2+\cdot+\hat \beta_kX_k+\hat u$</p><p><strong>OLS估计方法</strong>：找到一组{$\beta_1,\cdots,\beta_k$},使得残差的平方和最小；</p><script type="math/tex; mode=display">min Q=\sum_{i=1}^n{\hat u_i}^2=\sum_{i=1}^n(Y_i-\hat Y_i)^2</script><p>通过一阶条件为零，我们可以得到针对$k+1$个未知数的$k+1$的方程，解出$\hat\beta_0$、$\hat\beta_1$、$\cdots$、$\hat\beta_k$.</p><h4 id="拟合优度"><a href="#拟合优度" class="headerlink" title="拟合优度"></a>拟合优度</h4><p>跟简单回归模型一样，我们用拟合优度来衡量x究竟多好的结识了y。回归的$R^2$被定义为：</p><script type="math/tex; mode=display">R^2=\frac{SSE}{SST}=1-\frac{SSR}{SST}</script><p>SSR总是小于SST，因此$R^2$总是介于0和1之间。可以证明，$R^2$于$r^2$在数值上是相等的（这里只用简单回归的拟合优度作为例子，多元回归的拟合优度同理）：</p><script type="math/tex; mode=display">R^2=\frac{SSE}{SST}=\frac{\sum_{i=1}^n(\hat{y_i}-\bar y)^2}{\sum_{i=1}^n (y_i-\bar y)^2}=\frac{\sum_{i=1}^n(\hat \beta_1 x_i-\hat \beta_1 \bar x)^2}{\sum_{i=1}^n(y_i-\bar y)^2}=\hat \beta_1\frac{S_x^2}{S_y^2}=\frac{S_{xy}^2S_x^2}{S_x^4S_y^2}=(\frac{S_{xy}}{S_xS_y})^2=r^2</script><h4 id="估计量的无偏性"><a href="#估计量的无偏性" class="headerlink" title="估计量的无偏性"></a>估计量的无偏性</h4><p>为了证明OLS估计量的无偏性，我们需要如下假定：</p><ul><li><p>MLR1（线性于参数）：总体模型是参数的线性函数；</p></li><li><p>MLR2（随机抽样）：从总体中随机抽取样本容量为n的样本</p></li><li><p>MLR3（不完全共线性）：自变量不为常数且不是其他自变量的线性组合；</p></li><li><p>MLR4（零条件均值假定）：$E(u|x_i,x_2,\cdots,x_n)=0$</p></li></ul><p><strong>定理一：在假定MLR1-MLR4下，对$\forall j=0,1,\cdots,k ,\textbf{都有} E(\hat \beta_j)=\beta_j$，即OLS估计量是总体参数的无偏估计量</strong></p><h4 id="估计量的方差"><a href="#估计量的方差" class="headerlink" title="估计量的方差"></a>估计量的方差</h4><p>为了求出估计的方差我们还需要假定：</p><ul><li>MLR5（同方差性）：$Var(u|x_1,\cdots,x_k)=\sigma^2$</li></ul><p><strong>定理二（高斯—马尔可夫定理）t\beta_j$是$\beta_j$的无偏估计量（BLUES)</strong></p><p>根据MLR5,可以求得方差的无偏估计为$Var(\hat\beta_j)=\frac{\sigma^2}{SST_J(1-R_j^2)}$</p><h3 id="遗漏变量的偏误"><a href="#遗漏变量的偏误" class="headerlink" title="遗漏变量的偏误"></a>遗漏变量的偏误</h3><p>原模型：$\hat y=\hat\beta_0+\hat \beta x_1+\hat\beta_2x_2$</p><p>遗漏$x_2$后：$\tilde y=\tilde \beta_0+\tilde \beta_1x_1$</p><p>$\tilde\beta_1=\hat\beta_1+\hat\beta_2\tilde\delta_1$,$\delta_1=\frac{Cov(x_1,x_2)}{\sigma_1^2}$</p><p>如果原模型满足MLR1-MLR4,那么$E(\tilde\beta_1)=E(\hat\beta_1+\hat\beta_2\tilde\delta_1)=E(\hat\beta_1)+E(\hat\beta_2)\tilde\delta_1$</p><p>所以有$Bias(\tilde\beta_1)=E(\tilde\beta_1)-\beta_1=\beta_2\tilde\delta_1$</p><div class="table-container"><table><thead><tr><th></th><th>$Cov(x_1,x_2)&gt;0$</th><th>$Cov(x_1,x_2)&gt;0$</th></tr></thead><tbody><tr><td>$\beta_2&gt;0$</td><td>偏误为正</td><td>偏误为负</td></tr><tr><td>$\beta_2&lt;0$</td><td>偏误为负</td><td>偏误为正</td></tr></tbody></table></div><h3 id="模型中包含无关变量"><a href="#模型中包含无关变量" class="headerlink" title="模型中包含无关变量"></a>模型中包含无关变量</h3><p>$E(y|x_1,x_2,x_3)=E(y|x_1,x_2)=\beta_0+\beta_1x_1+\beta_2x_2$</p><p>可以看到均值并不会有什么影响，根据OLS无偏性可知$E(\hat \beta_3)=\beta_3=0$，但无关变量会对估计量的方差有影响，它会加剧多重共线性使估计量的方差变大。$Var(\hat\beta_j)=\frac{\sigma^2}{SST_J(1-R_j^2)}$，若$R_j^2=1$,则说明模型完全共线性，$R_j^2\rightarrow 1$则被成为多重共线性。</p><p>同样的，系数的标准差就为$sd(\hat\beta_j)=\frac{\sigma}{[SST_J(1-R_j^2)]^{1/2}}$，由于$\sigma$是未知的，我们用标准误$\hat\sigma$来代替,这样便有$se(\hat\beta_j)=\frac{\hat \sigma}{[SST_J(1-R_j^2)]^{1/2}}$.</p>]]></content>
      
      
      <categories>
          
          <category> 计量经济学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>简单回归模型（二）</title>
      <link href="2021/03/19/%E7%AE%80%E5%8D%95%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>2021/03/19/%E7%AE%80%E5%8D%95%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>简单回归模型可以用来研究两个变量之间的关系。出于我们就将会看到的原因，简单回归模型要作为经验分析的一般工具，还存在着局限性。不过有时把它作为一个经验工具还是很合适的。</p><span id="more"></span><h2 id="简单回归模型（二）"><a href="#简单回归模型（二）" class="headerlink" title="简单回归模型（二）"></a>简单回归模型（二）</h2><h3 id="拟合优度"><a href="#拟合优度" class="headerlink" title="拟合优度"></a>拟合优度</h3><p>已知：$y_i=\hat y_i+\hat u_i$,可以把OLS分成拟合值和残差两个部分，在样本中，拟合值和残差是不相关的。定义<strong>总平方和（SST）</strong>、<strong>解释平方和（SSE）</strong>，和<strong>残差平方和（SSR）</strong>如下：</p><script type="math/tex; mode=display">SST=\sum_{i=1}^{n}(y_i-\bar y)^2</script><script type="math/tex; mode=display">SSR=\sum_{i=1}^{n}(\hat y_i-\bar y)^2</script><script type="math/tex; mode=display">SSR=\sum_{i=1}^{n}u_i^2=\sum_{i-1}^{n}(y_i-\bar y)^2</script><p>容易证得：$SST=SSE+SSR$</p><p>假定总平方和SST不为零，我们可以通过将方程两边同时除以SST得到$1=SSE/SST+SSR/SST$。回归的$R^2$有时又被称为判定系数，是被解释波动与总波动之比，被定义为：</p><script type="math/tex; mode=display">R^2=SSE/SST=1-SSR/SST</script><p>根据该方程，$R^2$的值总是介于0和1之间。若所有数据点都落在同一条直线上，此时$R^2=1$;同理，一个接近于零的$R^2$给出了一个糟糕的拟合，因为$y_i$的波动极少能被$\bar y$所解释（后者全部落在OLS回归线上）。另外，可以证明$R^2$等于$y_i$和$x_i$样本相关系数的平方：</p><script type="math/tex; mode=display">R^2=\frac{\sum_{i=1}^n\hat y_i^2}{\sum_{i=1}^ny_i^2}=\frac{\hat \beta_1^2\sum_{i=1}^n x_i^2}{\sum_{i=1}^ny_i^2}=\cdots=r^2</script><p>(太麻烦了不写了)</p><p>注意：$R^2$不能作为评价计量分析成功与否的主要准则！</p><h3 id="度量单位与函数形式"><a href="#度量单位与函数形式" class="headerlink" title="度量单位与函数形式"></a>度量单位与函数形式</h3><p>当自变量和因变量的单位发生变化时，OLS的估计值也会发生变化。一般的自变量被除以或乘以一个非零的常数c时，OLS的斜率和截距也会分别被乘以或除以c，但判定系数$R^2$不会因y或x的单位变化而变化。</p><p>我们也可以把许多非线性因素引入到简单回归分析之中，例如：对于一个变量为非线性形式的模型：$Y=\beta_0X^\beta_1$,我们可以通过一定的函数变换得到一个新的模型，如$lnY=ln\beta_0+\beta_1lnX$,令$W=lnY$，$Z=lnX$,$\alpha_0=ln\beta_0$，就能够得到$W=\alpha_0+\beta_1Z$。再对上式进行OLS估计，可以得到$\beta_1$的无偏估计量$\hat \beta_1$和$\alpha_0$的无偏估计量$\hat \alpha_0$。</p><p>但是通过上述变换却无法得到$\beta_1$的无偏估计量$\hat \beta_1$，因为$e^{\hat \alpha_0}$并不是$\beta_0$的一个好的估计量。然而，在回归分析中一般都关心斜率系数而不是截距系数，因此我们只要能够正确估计出$\beta_1$，仍然可以看作是一个线性模型。</p><p>对解释变量进行某种形式的函数变换，不会改变模型的参数线性，但会使得模型的经济意义更加合理。主要有一下三种常用的函数形式：</p><div class="table-container"><table><thead><tr><th>模型</th><th>因变量</th><th>自变量</th><th>对$\beta_1$的解释</th></tr></thead><tbody><tr><td>水平值-水平值</td><td>y</td><td>x</td><td>$\triangle y=\beta_1\triangle x$</td></tr><tr><td>水平值-对数</td><td>y</td><td>log(x)</td><td>$\triangle y=(\beta_1/100)\%\triangle x$</td></tr><tr><td>对数-水平值</td><td>log(y)</td><td>x</td><td>$\%\triangle y=(100\beta_1)\triangle x$</td></tr><tr><td>对数-对数</td><td>log(y)</td><td>log(x)</td><td>$\%\triangle y=\beta_1\%\triangle x$</td></tr></tbody></table></div><h3 id="过原点回归"><a href="#过原点回归" class="headerlink" title="过原点回归"></a>过原点回归</h3><p>在分析问题时，有些情况我们希望x=0时y=0，这时得到的方程$\bar y=\bar \beta_1 x$又被称为<strong>过原点回归</strong>。通过最小化残差平方和的一阶条件，我们可以得到</p><script type="math/tex; mode=display">\bar \beta_1=\frac{\sum_{i=1}^nx_iy_i}{\sum_{i=1}^nx_i^2}</script><p>(分子不为零)</p><p>同样的，判定系数 </p><script type="math/tex; mode=display">R^2=1-\frac{\sum_{i=1}^n(y_i-\bar \beta_1x_i)^2}{\sum_{i=1}^ny_i^2}</script><p>使用过原点回归需要非常强的前提条件，在实际分析中应用不多。</p><h3 id="估计量的性质"><a href="#估计量的性质" class="headerlink" title="估计量的性质"></a>估计量的性质</h3><h4 id="高斯-马尔科夫假定"><a href="#高斯-马尔科夫假定" class="headerlink" title="高斯-马尔科夫假定"></a>高斯-马尔科夫假定</h4><p>定理：对于线性回归模型，在某些约束条件下，由最小二乘法得到的估计量（估计子），即线性回归模型的系数，是最优的线性无偏估计子。</p><p>为了判断点估计的无偏性、有效性等性质，需要对模型做出一些假定</p><p>1.线性于参数：$y=\beta_0+\beta_1x+u$</p><p>2.随机抽样：从总体中随机抽取样本，样本容量为n。同时，可以把随机样本的形式将方程写为：$y_i=\beta_0+\beta_1x_i+u_i$,其中$u_i$是第i次观测时的误差或干扰。</p><p>3.解释变量的样本有波动：即$x_i$的标准差不为零。</p><p>4.零条件均值假定：对给定任何x值，误差的期望都为零：$E(u|x)=0$</p><h4 id="OLS估计量的无偏性"><a href="#OLS估计量的无偏性" class="headerlink" title="OLS估计量的无偏性"></a>OLS估计量的无偏性</h4><p>基于以上四个假定，可以证明出OLS估计量的无偏性：</p><p>首先，我们把$\beta_1$的估计量改写为</p><script type="math/tex; mode=display">\hat \beta_1=\frac{\sum_{i=1}^n(x_i-\bar x)y_i}{\sum_{i=1}^n(x_i-\bar x)^2}</script><p>将随驾抽样形式的方程代入：其中 $SST_X=\sum_{i=1}^n(x_i-\bar x)^2$</p><script type="math/tex; mode=display">\hat \beta_1=\frac{\sum_{i=1}^n(x_i-\bar x)(\beta_0+\beta_1x_i+u_i)}{SST_x}=\frac{\beta_0\sum_{i=1}^n(x_I-\bar x)+\beta_1\sum_{i=1}^n(x_i-\bar x)x_i+\sum_{i=1}^n(x_i-\bar x)u_i}{SST_x}=\frac{\beta_1SST_x+\sum_{i=1}^n(x_i-\bar x)u_i}{SST_x}</script><p>进一步整理，得到 </p><script type="math/tex; mode=display">\hat \beta_1=\beta_1+(1/SST_x)\sum_{i=1}^n d_i u_i</script><p>其中 $d_i=x_i-\bar x$</p><p>得到上式，就可证明出OLS的无偏性：</p><p><strong>定理：</strong>基于假定1-4，对任意$\beta_1$和$\beta_0$都有 $E(\hat \beta_0)=\beta_0$,$E(\hat \beta_1)=\beta_1$。</p><script type="math/tex; mode=display">\begin{array}{c}\mathrm{E}\left(\hat{\beta}_{1}\right)=\beta_{1}+\mathrm{E}\left[\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} d_{i} u_{i}\right]=\beta_{1}+\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} \mathrm{E}\left(d_{i} u_{i}\right) \\=\beta_{1}+\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} d_{i} \mathrm{E}\left(u_{i}\right)=\beta_{1}+\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} d_{i} \cdot 0=\beta_{1}\end{array}</script><script type="math/tex; mode=display">\begin{aligned}&\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1} \bar{x}=\beta_{0}+\beta_{1} \bar{x}+\bar{u}-\hat{\beta}_{1} \bar{x}=\beta_{0}+\left(\beta_{1}-\hat{\beta}_{1}\right) x+u\\&\text { 据假定} 2 \text { 和 } 4, \text { 有 } \mathrm{E}(\bar{u})=0, \text { 于是以 } x_{i} \text { 的值为条 }\\&\mathrm{E}\left(\hat{\beta}_{0}\right)=\beta_{0}+\mathrm{E}\left[\left(\beta_{1}-\hat{\beta}_{1}\right) \bar{x}\right]+\mathrm{E}(\bar{u})=\beta_{0}+\mathrm{E}\left[\left(\beta_{1}-\hat{\beta}_{1}\right)\right] \bar{x}=\beta_1\end{aligned}</script><p>证毕。</p><h4 id="OLS估计量的方差"><a href="#OLS估计量的方差" class="headerlink" title="OLS估计量的方差"></a>OLS估计量的方差</h4><p>结果表明，在假定1至4下，OLS估计量的方差可以计算出来。不过，这些表达式多少有些复杂。有鉴于此，我们增加一个在横截面分析中的传统假定。这个假定要求，以x为条件，无法观测变量u的方差是一一个常数。这就是同方差(homoskedasticity) 或“常方差”假定。</p><p>假定5（同方差性）：对任意x，误差都具有相同的方差，即$Var(u|x)=\sigma^2$（误差方差）</p><p><img src="/2021/03/19/%E7%AE%80%E5%8D%95%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89/Blog\source\_posts\简单回归模型（二）\同方差性.PNG" alt="同方差下的简单回归分析"></p><p>相反的，当$Var(u|x)$取决于x时，便称误差项表现为<strong>异方差性</strong>。由于$Var(u|x)=Var(y|x)$,所以只要$Var(y|x)$是x的函数，便表现出异方差性。</p><p><img src="/2021/03/19/%E7%AE%80%E5%8D%95%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89/Blog\source\_posts\简单回归模型（二）\异方差性.PNG" alt="异方差性下的简单回归分析"></p><p><strong>定理：OLS估计量的抽样方差</strong></p><p>在假定1-5下，我们有</p><script type="math/tex; mode=display">Var(\hat \beta_1)=\sigma^2/SST_x</script><script type="math/tex; mode=display">Var(\hat \beta_0)=\frac{\sigma^2n^{-1}\sum_{i=1}^nx_i^2}{\sum_{i=1}^n(x_i-\bar x)^2}</script><p>证明：</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{Var}\left(\hat{\beta}_{1}\right) &=\left(1 / \operatorname{SST}_{x}\right)^{2} \operatorname{Var}\left(\sum_{i=1}^{n} d_{i} u_{i}\right)=\left(1 / \mathrm{SST}_{x}\right)^{2}\left(\sum_{i=1}^{\infty} d^{\prime} \operatorname{Var}\left(u_{i}\right)\right) \\&=\left(1 / \mathrm{SST}_{x}\right)^{2}\left(\sum_{i=1}^{n} d_{i}^{2} \sigma^{2}\right)\left[\text { 因为 } \operatorname{Var}\left(u_{i}\right)=\sigma^{2}, \forall i\right] \\&=\sigma^{2}\left(1 / \mathrm{SST}_{x}\right)^{2}\left(\sum_{i=1}^{n} d_{i}^{2}\right)=\sigma^{2}\left(1 / \mathrm{SST}_{x}\right)^{2} \mathrm{SST},=\sigma / \mathrm{SST}_{x}\end{aligned}</script><p>（$Var(\hat \beta_0)$同理）</p><p>证毕</p><p>我们可以看出，误差方差越大，$Var(\hat \beta_1)$就越大：影响y的不可观测因素波动越大，要准确估计$\beta_1$也就越难；自变量波动越大，$Var(\hat \beta_1)$也就越小，x的样本越分散，越容易估计出$\beta_1$，因此，较大的样本容量会使$\hat \beta_1$的方差越小。</p><h4 id="误差方差的估计"><a href="#误差方差的估计" class="headerlink" title="误差方差的估计"></a>误差方差的估计</h4><p><strong>待更</strong></p><hr>]]></content>
      
      
      <categories>
          
          <category> 计量经济学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>简单回归模型</title>
      <link href="2021/03/12/%E7%AE%80%E5%8D%95%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
      <url>2021/03/12/%E7%AE%80%E5%8D%95%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>简单回归模型可以用来研究两个变量之间的关系。出于我们就将会看到的原因，简单回归模型要作为经验分析的一般工具，还存在着局限性。不过有时把它作为一个经验工具还是很合适的。</p><span id="more"></span><h2 id="简单回归模型"><a href="#简单回归模型" class="headerlink" title="简单回归模型"></a>简单回归模型</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>回归分析：在其他条件不变的情况下，考察一个变量对另一个变量的影响。</p><div class="table-container"><table><thead><tr><th style="text-align:center">X</th><th style="text-align:center">自变量</th><th style="text-align:center">解释变量</th></tr></thead><tbody><tr><td style="text-align:center">Y</td><td style="text-align:center">因变量</td><td style="text-align:center">被解释变量</td></tr></tbody></table></div><p>设变量u表示关系式中的干扰项，表示除X之外其他影响Y的因素。</p><p>我们用一个简单的方程来表示它们之间的关系：</p><script type="math/tex; mode=display">Y=\beta_0+\beta_1 x+u</script><p>当X发生变化时，$\triangle Y=\beta_1\triangle X+\triangle u$,如果$\triangle u=0$,那么$\triangle Y=\beta_1\triangle X$,从而可以用$\beta_1$衡量X对Y的影响。</p><h4 id="零条件均值假定"><a href="#零条件均值假定" class="headerlink" title="零条件均值假定"></a>零条件均值假定</h4><p>如何保证其他条件不变？简单地，如果X和u是独立的，即X的变化不会对u造成系统性影响，那么$\beta_1$就可以度量其他条件不变的情况下X对Y的影响。在计量分析中，采用一个更弱的技术性假定——<strong>零条件均值假定</strong></p><p>首先，对于$Y=\beta_0+\beta_1 x+v$，若$E(v)=a=0$，令u=v;若$E(v)=a\neq0$,令$u=v-a$,这样使$E(u)=0$，这样变换后的方程为$Y=(\beta_0+a)+\beta_1 x+u$使得干扰项的均值为0.</p><p>因为u和x是随机变量，所以我们能在任何给定x下定义u的条件分布，所以关键假设是<strong>u的均值与x无关</strong>。写作:$E(u|x)=E(u)$</p><p>该方程表示：u的均值独立于x，（用均值独立来近似说明u独立于x）结合$E(u)=0$，就得到了零条件均值假定：$E(u|x)=0$.</p><p>零条件均值假定的直观含义：由于误差项的存在，x对y的影响是随机的。但如果零条件均值假定成立，那么无论x取什么值，误差项对y的平均影响为零，从而x对y的均值的影响是确定性的。换言之，我们无法确定x与y的关系，但可以确定x与y的均值之间的关系。</p><h4 id="总体回归函数"><a href="#总体回归函数" class="headerlink" title="总体回归函数"></a>总体回归函数</h4><p>根据零条件均值假定：</p><script type="math/tex; mode=display">E(y|x)=E(\beta_0+\beta_1 x+u|x)</script><script type="math/tex; mode=display">=E(\beta_0|x)+E(\beta_1 x|x)+E(u|x)=\beta_0+\beta_1 x</script><script type="math/tex; mode=display">E(y|x)=\beta_0+\beta_1 x</script><p>被称为总体回归函数。$\triangle E(y|x)=\beta_1$,因此$\beta_1$衡量了x增加一个单位对y的条件均值的影响。</p><p>进而推得$y=E(y|x)+u$,该方程把y分成两部分，一部分是$E(y|x)$,被称为y的系统部分，可以由x解释；另一部分u被称为非系统部分，不能被x解释，但它的均值为0。</p><p><img src="/2021/03/12/%E7%AE%80%E5%8D%95%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/Users\legion\Desktop\新建文件夹\VSCODE\计量经济学\简单回归模型1.PNG" alt></p><h3 id="普通最小二乘法-OLS"><a href="#普通最小二乘法-OLS" class="headerlink" title="普通最小二乘法(OLS)"></a>普通最小二乘法(OLS)</h3><h4 id="矩估计"><a href="#矩估计" class="headerlink" title="矩估计"></a>矩估计</h4><p>接下来讨论如何估计参数$\beta_0$和$\beta_1$,我们通过矩估计的方法，用样本矩估计总体矩。令{$(x_i,y_i):(i=1,2,\cdots,n)$}表示从总体中抽取容量为n的样本，对每个i，都有</p><script type="math/tex; mode=display">y_i=\beta_0+\beta_1 x_i +u_i</script><p>其中$u_i$为第i次观测的干扰项。</p><p>根据零条件均值假定，我们知道$E(u)=0$,$Cov(x,u)=0$,所以有 $Cov(x,u)=E(xu)-E(x)E(u)=E(xu)=0$</p><p>所以 $E(u)=E(y-\beta_0-\beta_1 x)=0$</p><p>$E(ux)=E[x(y-\beta_0-\beta_1 x)]=0$</p><p>用样本均值代替总体均值，选择估计值$\hat \beta_0$和$\hat \beta_1$来代替$\beta_0$和$\beta_1$，以上两式就可以写成:</p><script type="math/tex; mode=display">\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat \beta_0 -\hat \beta_1 x)=0</script><script type="math/tex; mode=display">\frac{1}{n}\sum_{i=1}^{n}x_i(y_i-\hat \beta_0 -\hat \beta_1 x)=0</script><p>对于等式一，可以改写为 $\hat\beta_0=\bar y-\hat\beta_1 \bar x$</p><p>对于等式二，做进一步的替换：</p><script type="math/tex; mode=display">\sum_{i=1}^{n}x_i(y_i-\hat \beta_0 -\hat \beta_1 x)=0</script><script type="math/tex; mode=display">\sum_{i=1}^{n}x_i[y_i-(\bar y-\hat \beta_1 \bar x) -\hat \beta_1 x]=0</script><script type="math/tex; mode=display">\sum_{i=1}^{n}x_i(y_i-\bar y)=\hat \beta_1\sum_{i=1}^{n}x_i(x_i-\bar x)</script><p>根据求和运算的性质，有</p><script type="math/tex; mode=display">\sum_{i-1}^n(x_i-\bar x)^2=\sum_{i=1}^{n}(x_i^2-2x_i\bar x+\bar x^2)</script><script type="math/tex; mode=display">=\sum_{i=1}^n(x_i^2-x_i\bar x)=\sum_{i=1}^{n}x_i(x_i-\bar x)</script><p>同理，$\sum_{i-1}^n x_i(y_i-\bar y)=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)$</p><p>所以只要有$\sum_{i-1}^n(x_i-\bar x)^2&gt;0$ 就有$\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum_{i-1}^n(x_i-\bar x)^2}$</p><p>根据代数知识，$\hat \beta_1=\frac{Cov(x,y)}{S_x^2}=\frac{Cov(x,y)}{S_xS_y}\cdot \frac{s_y}{S_x}=\hat r_{xy}\cdot \frac{S_y}{S_x}$</p><p>由样本推得总体：$\beta_1=r_{xy}\cdot\frac{S_y}{S_x}$</p><p>可以看出，若x与y正相关，则斜率为正；若x与y负相关，则斜率为负。但是，简单回归本质上是两个变量之间的相关性分析，所以在推导因果关系时需要非常小心。</p><h4 id="最小化残差平方和"><a href="#最小化残差平方和" class="headerlink" title="最小化残差平方和"></a>最小化残差平方和</h4><p>对任意斜率和截距$\beta_0$和$\beta_1$,定义y在$x=x_i$时的一个拟合值为</p><script type="math/tex; mode=display">\hat y_i=\hat\beta_0+\hat\beta_1 x_i</script><p>这是在给定斜率和截距下，y在$x=x_i$时的预测值。样本中每一次观测都有一个拟合值，第i次观测的残差就是其实际值与拟合值之差：$u_i=y_i-\hat\beta_0-\hat\beta_1 x_i$</p><p>事实上，普通最小二乘法之所以得名，就是因为$\hat\beta_0,\hat\beta_1$这些估计值最小化了残差的平方和：</p><script type="math/tex; mode=display">\sum_{i=1}^{n}\hat u_i^2=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1 x_i)^2</script><p>其一阶条件恰为</p><p>$\sum_{i=1}^{n}(y_i-\hat \beta_0 -\hat \beta_1 x)=0 \sum_{i=1}^{n}x_i(y_i-\hat \beta_0 -\hat \beta_1 x)=0$</p><p>一旦确定了截距和斜率的估计值，就能够建立OLS回归线：</p><p>$\hat y=\hat \beta_0+\hat \beta_1 x$  从该方程中得到的预测值便是估计值。</p><p>该方程又被称作<strong>样本回归函数</strong>，因为它是总体回归函数<script type="math/tex">E(y|x)=\beta_0+\beta_1 x</script>的一个样本估计。（总体回归函数是唯一且未知的）样本回归函数来自于给定一组数据的样本，所以对于不同的样本，OLS回归线有不同的斜率和截距。</p><p>在大多数情形中，斜率的估计值可以写成：$\hat\beta_1=\triangle\hat y/\triangle x$,它告诉我们x变化一个单位时的$\hat y$的变化量；</p><p>类似的，有$\triangle \hat y=\hat \beta_1\triangle x$,所以在给定x的一个变化，我们都能计算出y的预期变化。</p>]]></content>
      
      
      <categories>
          
          <category> 计量经济学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/03/10/hello-world/"/>
      <url>2021/03/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> hello world </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hello world </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
